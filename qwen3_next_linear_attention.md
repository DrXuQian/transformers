# Qwen3-Next Linear Attention (Gated Delta Net) è¯¦è§£

## ğŸ“š 1. ä»€ä¹ˆæ˜¯Linear Attentionï¼Ÿ

### ä¼ ç»Ÿæ³¨æ„åŠ› vs çº¿æ€§æ³¨æ„åŠ›

| ç‰¹æ€§ | ä¼ ç»Ÿæ³¨æ„åŠ›ï¼ˆSoftmax Attentionï¼‰ | çº¿æ€§æ³¨æ„åŠ›ï¼ˆLinear Attentionï¼‰ |
|------|----------------------------------|--------------------------------|
| **æ—¶é—´å¤æ‚åº¦** | O(NÂ²) - Næ˜¯åºåˆ—é•¿åº¦ | O(N) - çº¿æ€§å¤æ‚åº¦ï¼ |
| **ç©ºé—´å¤æ‚åº¦** | O(NÂ²) | O(1) |
| **è®¡ç®—æ–¹å¼** | Attention = softmax(QK^T/âˆšd)V | é€’å½’/ç´¯ç§¯æ›´æ–° |
| **æ³¨æ„åŠ›çŸ©é˜µ** | éœ€è¦å­˜å‚¨å®Œæ•´çš„NÃ—NçŸ©é˜µ | æ— éœ€æ˜¾å¼çŸ©é˜µ |
| **é•¿åºåˆ—æ€§èƒ½** | å†…å­˜å’Œè®¡ç®—å¼€é”€å·¨å¤§ | å¯å¤„ç†100k+é•¿åº¦ |
| **å®ç°æ–¹å¼** | æ ‡å‡†çŸ©é˜µè¿ç®— | Gated Delta Net |

## ğŸ—ï¸ 2. Qwen3-Nextçš„Gated Delta Netæ¶æ„

### æ ¸å¿ƒç»„ä»¶é…ç½®

#### æŠ•å½±ç»´åº¦
```python
# é…ç½®å‚æ•°
linear_key_head_dim = 128      # Kå¤´ç»´åº¦
linear_value_head_dim = 128    # Vå¤´ç»´åº¦
linear_num_key_heads = 16      # Kå¤´æ•°é‡
linear_num_value_heads = 32    # Vå¤´æ•°é‡

# è®¡ç®—æ€»ç»´åº¦
key_dim = 128 Ã— 16 = 2048
value_dim = 128 Ã— 32 = 4096
```

#### å·ç§¯ç»„ä»¶
- **å·ç§¯æ ¸å¤§å°**: `linear_conv_kernel_dim = 4`
- **ç±»å‹**: 1Dæ·±åº¦å·ç§¯ï¼ˆdepthwiseï¼‰
- **ç‰¹ç‚¹**: æ¯ä¸ªé€šé“ç‹¬ç«‹å·ç§¯ï¼ˆgroups=channelsï¼‰
- **ä½œç”¨**: åºåˆ—å»ºæ¨¡ï¼Œæ•è·å±€éƒ¨ä¾èµ–

#### é—¨æ§æœºåˆ¶
- **Betaé—¨ï¼ˆÎ²ï¼‰**: æ§åˆ¶ä¿¡æ¯æµåŠ¨
- **Alphaé—¨ï¼ˆÎ±ï¼‰**: æ§åˆ¶è¡°å‡ç‡
- **Zé—¨**: ç”¨äºå½’ä¸€åŒ–

## âš™ï¸ 3. æ ¸å¿ƒè®¡ç®—æµç¨‹

### Step 1: è¾“å…¥æŠ•å½±

```
hidden_states [B, L, D]
         â†“
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
QKVZæŠ•å½±   BAæŠ•å½±
    â”‚         â”‚
    â†“         â†“
Q,K,V,Z    Beta,Alpha
```

**QKVZæŠ•å½±**: `Linear(D, 2*key_dim + 2*value_dim)`
- Query: `[B, L, 16, 128]`
- Key: `[B, L, 16, 128]`
- Value: `[B, L, 32, 128]`
- Z: `[B, L, 32, 128]`

**BAæŠ•å½±**: `Linear(D, 2*num_v_heads)`
- Beta: `[B, L, 32]` â†’ sigmoidæ¿€æ´»
- Alpha: `[B, L, 32]` â†’ è®¡ç®—è¡°å‡ç‡

### Step 2: å› æœå·ç§¯

```
QKVæ··åˆ â†’ Conv1D(kernel=4, causal) â†’ æ¿€æ´»(SiLU)
```

**ä½œç”¨**ï¼š
- æ•è·å±€éƒ¨ä¾èµ–å…³ç³»
- ä¿æŒå› æœæ€§ï¼ˆåªçœ‹è¿‡å»ä¿¡æ¯ï¼‰
- å¢å¼ºåºåˆ—å»ºæ¨¡èƒ½åŠ›

### Step 3: Gated Delta Ruleï¼ˆæ ¸å¿ƒï¼ï¼‰

#### A. Chunkæ¨¡å¼ï¼ˆè®­ç»ƒ/é•¿åºåˆ—ï¼‰
```python
chunk_gated_delta_rule(Q, K, V, g, beta)
```
- å°†åºåˆ—åˆ†å—å¤„ç†
- å—å†…å¹¶è¡Œè®¡ç®—
- å—é—´é€’å½’ä¼ é€’çŠ¶æ€

#### B. Recurrentæ¨¡å¼ï¼ˆæ¨ç†/å•tokenï¼‰
```python
recurrent_gated_delta_rule(Q, K, V, g, beta, state)
```
- é€tokené€’å½’æ›´æ–°
- ç»´æŠ¤ç´¯ç§¯çŠ¶æ€
- é€‚åˆè‡ªå›å½’ç”Ÿæˆ

## ğŸ”¬ 4. Gated Delta Ruleæ•°å­¦åŸç†

### æ ¸å¿ƒå…¬å¼

#### 1. è¡°å‡é—¨è®¡ç®—
```python
g = -exp(A_log) * softplus(alpha + dt_bias)
```
- `A_log`: å¯å­¦ä¹ çš„è¡°å‡å‚æ•°
- `alpha`: è¾“å…¥ç›¸å…³çš„è¡°å‡è°ƒèŠ‚
- `dt_bias`: æ—¶é—´æ­¥åç½®

#### 2. ä¿¡æ¯é—¨
```python
beta = sigmoid(b)
```
æ§åˆ¶æ–°ä¿¡æ¯çš„æ¥å—ç¨‹åº¦

#### 3. é€’å½’æ›´æ–°ï¼ˆç®€åŒ–ç‰ˆï¼‰
```python
# åˆå§‹åŒ–
state = 0

# å¯¹æ¯ä¸ªæ—¶é—´æ­¥t:
state = g[t] * state + beta[t] * (k[t] âŠ— v[t])
output[t] = q[t] Â· state
```

#### 4. L2å½’ä¸€åŒ–
- Qå’ŒKåœ¨è®¡ç®—å‰è¿›è¡ŒL2å½’ä¸€åŒ–
- ç¡®ä¿æ•°å€¼ç¨³å®šæ€§

### å®é™…å®ç°ç‰¹æ€§
- å¤šå¤´å¹¶è¡Œå¤„ç†
- å—çº§ä¼˜åŒ–
- èåˆç®—å­åŠ é€Ÿ

## ğŸš€ 5. æ€§èƒ½ä¼˜åŠ¿

### å¤æ‚åº¦å¯¹æ¯”

| æ“ä½œ | ä¼ ç»Ÿæ³¨æ„åŠ› | Linear Attention | ä¼˜åŠ¿å€æ•° |
|------|-----------|------------------|----------|
| **æ—¶é—´å¤æ‚åº¦** | O(NÂ²) | O(N) | Nå€ |
| **ç©ºé—´å¤æ‚åº¦** | O(NÂ²) | O(1) | NÂ²å€ |
| **KV Cache** | O(NÃ—D) | O(D) | Nå€ |

### å®é™…æ€§èƒ½æå‡

| åºåˆ—é•¿åº¦ | æ€§èƒ½æå‡ | å†…å­˜èŠ‚çœ |
|---------|---------|----------|
| 1k | ~10x | ~1000x |
| 10k | ~100x | ~100,000x |
| 100k | ~10,000x | ~10,000,000x |

### é€‚ç”¨åœºæ™¯
- âœ… è¶…é•¿æ–‡æ¡£å¤„ç†
- âœ… æµå¼æ¨ç†
- âœ… å†…å­˜å—é™ç¯å¢ƒ
- âœ… å®æ—¶ç”Ÿæˆä»»åŠ¡

## ğŸ’¡ 6. å®ç°ç»†èŠ‚

### ç±»ç»“æ„
```python
class Qwen3NextGatedDeltaNet(nn.Module):
    def __init__(self, config: Qwen3NextConfig, layer_idx: int):
        # æ›¿ä»£ä¼ ç»Ÿçš„MultiHeadAttention
        # æ¯ä¸ªdecoderå±‚å¯é€‰æ‹©ä½¿ç”¨
```

### ä¼˜åŒ–å®ç°
1. **èåˆç®—å­**
   - ä½¿ç”¨FLAåº“çš„èåˆç®—å­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
   - å›é€€åˆ°PyTorchçº¯å®ç°
   - Causal Conv1Dä¸“ç”¨CUDAæ ¸

2. **çŠ¶æ€ç¼“å­˜**
   ```python
   conv_states: [B, C, K-1]      # å·ç§¯çŠ¶æ€
   recurrent_states: [B, H, D, D] # é€’å½’çŠ¶æ€
   ```
   - æ”¯æŒKV Cacheå…¼å®¹æ¥å£

3. **æ··åˆæ¶æ„**
   - å¯ä¸ä¼ ç»Ÿæ³¨æ„åŠ›å±‚äº¤æ›¿ä½¿ç”¨
   - ä¾‹å¦‚ï¼š`["linear", "linear", "softmax", "linear", ...]`
   - çµæ´»é…ç½®æ¯å±‚ç±»å‹

## ğŸ“Š 7. ä¸ä¼ ç»Ÿæ³¨æ„åŠ›çš„å¯¹æ¯”

| ç‰¹æ€§ | Softmax Attention | Gated Delta Net |
|-----|------------------|-----------------|
| **å¤æ‚åº¦** | O(NÂ²) | O(N) |
| **é•¿ç¨‹ä¾èµ–** | âœ… å®Œç¾ | âš ï¸ è¿‘ä¼¼ |
| **å¯è§£é‡Šæ€§** | âœ… æ³¨æ„åŠ›æƒé‡ | âŒ éšå¼çŠ¶æ€ |
| **è®­ç»ƒç¨³å®šæ€§** | âœ… æˆç†Ÿ | âš ï¸ éœ€è¦è°ƒä¼˜ |
| **æ¨ç†æ•ˆç‡** | âŒ æ…¢ | âœ… å¿« |
| **å†…å­˜æ•ˆç‡** | âŒ é«˜ | âœ… ä½ |
| **å¹¶è¡ŒåŒ–** | âœ… å®Œå…¨å¹¶è¡Œ | âš ï¸ å—çº§å¹¶è¡Œ |

### è®¾è®¡æƒè¡¡
- ç‰ºç‰²ä¸€å®šçš„è¡¨è¾¾èƒ½åŠ›æ¢å–æ•ˆç‡
- é€‚åˆéœ€è¦å¤„ç†è¶…é•¿åºåˆ—çš„åœºæ™¯
- åœ¨æŸäº›ä»»åŠ¡ä¸Šå¯èƒ½ç•¥é€Šäºä¼ ç»Ÿæ³¨æ„åŠ›

## ğŸ”§ 8. é…ç½®ç¤ºä¾‹

### Qwen3-Next-80Bé…ç½®

```json
{
    // Linear Attentioné…ç½®
    "linear_conv_kernel_dim": 4,
    "linear_key_head_dim": 128,
    "linear_value_head_dim": 128,
    "linear_num_key_heads": 16,
    "linear_num_value_heads": 32,

    // æ ‡å‡†Attentioné…ç½®ï¼ˆå¯¹æ¯”ï¼‰
    "num_attention_heads": 16,
    "num_key_value_heads": 2,  // GQA 8:1
    "hidden_size": 2048
}
```

### å±‚ç±»å‹é…ç½®
å¯é€šè¿‡`layer_types`æŒ‡å®šæ¯å±‚ä½¿ç”¨å“ªç§æ³¨æ„åŠ›ï¼š
```python
layer_types = ["linear", "linear", "standard", "linear", ...]
```

## ğŸ“ 9. ä»£ç å®ç°ç»†èŠ‚

### chunk_gated_delta_rule (è®­ç»ƒæ—¶ä½¿ç”¨)

```python
def chunk_gated_delta_rule(
    query,      # [B, L, H, D] - æŸ¥è¯¢å‘é‡
    key,        # [B, L, H, D] - é”®å‘é‡
    value,      # [B, L, H, D] - å€¼å‘é‡
    g,          # [B, L, H] - è¡°å‡é—¨
    beta,       # [B, L, H] - ä¿¡æ¯é—¨
    initial_state=None,
    output_final_state=False,
    use_qk_l2norm_in_kernel=True
):
    # 1. L2å½’ä¸€åŒ–Qå’ŒK
    if use_qk_l2norm_in_kernel:
        query = l2norm(query)
        key = l2norm(key)

    # 2. åˆ†å—å¤„ç†
    for chunk in chunks:
        # å—å†…å¹¶è¡Œè®¡ç®—
        state = update_state(chunk, g, beta)
        output = compute_output(query, state)

    return output, final_state
```

### recurrent_gated_delta_rule (æ¨ç†æ—¶ä½¿ç”¨)

```python
def recurrent_gated_delta_rule(
    query, key, value, g, beta,
    initial_state, ...
):
    state = initial_state

    # é€tokené€’å½’
    for t in range(seq_len):
        # çŠ¶æ€æ›´æ–°
        state = g[t] * state + beta[t] * outer(k[t], v[t])
        # è¾“å‡ºè®¡ç®—
        output[t] = dot(q[t], state)

    return output, state
```

### å› æœå·ç§¯å¤„ç†

```python
# å·ç§¯é…ç½®
self.conv1d = nn.Conv1d(
    in_channels=conv_dim,
    out_channels=conv_dim,
    kernel_size=4,          # å·ç§¯æ ¸å¤§å°
    groups=conv_dim,        # æ·±åº¦å·ç§¯
    padding=3,              # å› æœpadding
)

# åº”ç”¨å·ç§¯
mixed_qkv = self.causal_conv1d_fn(
    x=mixed_qkv,
    weight=self.conv1d.weight,
    activation="silu"       # SiLUæ¿€æ´»
)
```

## ğŸ” 10. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. èåˆç®—å­
- ä½¿ç”¨FLAåº“çš„CUDAæ ¸å¿ƒ
- å‡å°‘å†…å­˜è®¿é—®æ¬¡æ•°
- ç®—å­çº§ä¼˜åŒ–

### 2. æ··åˆç²¾åº¦
- FP16/BF16è®¡ç®—
- FP32ç´¯ç§¯
- æ¢¯åº¦ç¼©æ”¾

### 3. çŠ¶æ€ç®¡ç†
- å¢é‡æ›´æ–°è€Œéå®Œå…¨é‡ç®—
- é«˜æ•ˆçš„ç¼“å­˜æœºåˆ¶
- æœ€å°åŒ–å†…å­˜æ‹·è´

### 4. å¹¶è¡Œç­–ç•¥
- **å¤´å¹¶è¡Œ**: å¤šå¤´ç‹¬ç«‹è®¡ç®—
- **åºåˆ—å¹¶è¡Œ**: é•¿åºåˆ—åˆ†ç‰‡
- **å¼ é‡å¹¶è¡Œ**: æ¨¡å‹å¹¶è¡Œ

## ğŸ“Œ æ€»ç»“

Qwen3-Nextçš„Linear Attention (Gated Delta Net)æ˜¯ä¸€ä¸ªé‡è¦åˆ›æ–°ï¼š

1. **çº¿æ€§å¤æ‚åº¦**: O(N)æ—¶é—´å’ŒO(1)ç©ºé—´
2. **é—¨æ§æœºåˆ¶**: Betaé—¨å’Œè¡°å‡é—¨ç²¾ç¡®æ§åˆ¶ä¿¡æ¯æµ
3. **å› æœå·ç§¯**: å¢å¼ºå±€éƒ¨å»ºæ¨¡èƒ½åŠ›
4. **é€’å½’æ›´æ–°**: é«˜æ•ˆçš„çŠ¶æ€ä¼ é€’
5. **æ··åˆæ¶æ„**: å¯ä¸ä¼ ç»Ÿæ³¨æ„åŠ›çµæ´»ç»„åˆ

è¿™ä½¿å¾—Qwen3-Nextèƒ½å¤Ÿé«˜æ•ˆå¤„ç†è¶…é•¿åºåˆ—ï¼ˆ100k+ï¼‰ï¼Œä¸ºé•¿æ–‡æœ¬ç†è§£å’Œç”Ÿæˆä»»åŠ¡æä¾›äº†æ–°çš„å¯èƒ½æ€§ã€‚

## å‚è€ƒèµ„æ–™

- [Qwen3-Nextæ¨¡å‹ä»£ç ](./src/transformers/models/qwen3_next/modeling_qwen3_next.py)
- [FLAåº“](https://github.com/fla-org/flash-linear-attention)
- [Causal Conv1D](https://github.com/Dao-AILab/causal-conv1d)