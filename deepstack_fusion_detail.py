"""
DeepStackç‰¹å¾èåˆæœºåˆ¶è¯¦è§£
å±•ç¤ºLLMå¦‚ä½•ä¸Visionç‰¹å¾åœ¨ä¸åŒå±‚èåˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# ================== èåˆæ–¹å¼å®ç° ==================

class Qwen3VLModel(nn.Module):
    """
    Qwen3-VL ä¸»æ¨¡å‹ï¼Œå±•ç¤ºç‰¹å¾èåˆçš„è¯¦ç»†å®ç°
    """
    def __init__(self, config):
        super().__init__()
        self.vision_encoder = VisionEncoder(config)
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)

        # 36å±‚LLM Decoder
        self.layers = nn.ModuleList([
            DecoderLayer(config) for _ in range(36)
        ])

        # DeepStackç‰¹å¾æ˜ å°„å…³ç³»
        # Visionå±‚ -> LLMå±‚çš„æ˜ å°„
        self.deepstack_mapping = {
            5: list(range(0, 4)),    # Vision Layer 5 -> LLM Layer 0-3
            11: list(range(4, 8)),   # Vision Layer 11 -> LLM Layer 4-7
            17: list(range(8, 12))   # Vision Layer 17 -> LLM Layer 8-11
        }

    def forward(self, input_ids, pixel_values, attention_mask=None):
        """
        å®Œæ•´çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼Œå±•ç¤ºèåˆç»†èŠ‚
        """

        # ========== Step 1: è·å–Visionç‰¹å¾ ==========
        vision_outputs = self.vision_encoder(pixel_values)
        final_vision_features = vision_outputs['final']  # [seq_v, 2560]
        deepstack_features = vision_outputs['deepstack']  # {5: tensor, 11: tensor, 17: tensor}

        # ========== Step 2: å‡†å¤‡æ–‡æœ¬è¾“å…¥ ==========
        text_embeds = self.embed_tokens(input_ids)  # [seq_t, 2560]

        # ========== Step 3: è¾“å…¥å±‚èåˆï¼ˆæ‹¼æ¥æ–¹å¼ï¼‰==========
        # æ‰¾åˆ°ç‰¹æ®Šçš„<image>æ ‡è®°ä½ç½®
        image_token_mask = (input_ids == IMAGE_TOKEN_ID)

        # æ–¹å¼1ï¼šç›´æ¥æ›¿æ¢
        # å°†<image>ä½ç½®çš„embeddingæ›¿æ¢ä¸ºvision features
        if image_token_mask.any():
            # è·å–<image>çš„ä½ç½®
            image_positions = torch.where(image_token_mask)[0]

            # æ›¿æ¢ï¼šå°†vision featuresæ’å…¥åˆ°<image>ä½ç½®
            inputs_embeds = text_embeds.clone()
            inputs_embeds[image_positions[0]:image_positions[0]+len(final_vision_features)] = final_vision_features
        else:
            # æ–¹å¼2ï¼šåºåˆ—æ‹¼æ¥
            # [CLS] [Text Tokens] [Vision Features] [Text Tokens]
            inputs_embeds = torch.cat([text_embeds[:prefix_len],
                                      final_vision_features,
                                      text_embeds[prefix_len:]], dim=0)

        hidden_states = inputs_embeds  # [total_seq_len, 2560]

        # ========== Step 4: é€šè¿‡LLMå±‚ï¼Œé€å±‚èåˆDeepStackç‰¹å¾ ==========
        for layer_idx, decoder_layer in enumerate(self.layers):

            # æ ‡å‡†Transformerè®¡ç®—
            hidden_states = decoder_layer(
                hidden_states,
                attention_mask=attention_mask
            )

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ³¨å…¥DeepStackç‰¹å¾
            deepstack_feature = self.get_deepstack_feature_for_layer(
                layer_idx,
                deepstack_features,
                hidden_states.shape
            )

            if deepstack_feature is not None:
                # ğŸ”¥ æ ¸å¿ƒï¼šDeepStackç‰¹å¾èåˆ
                hidden_states = self.fuse_deepstack_features(
                    hidden_states,
                    deepstack_feature,
                    layer_idx
                )

        return hidden_states

    def get_deepstack_feature_for_layer(self, layer_idx, deepstack_features, target_shape):
        """
        è·å–å½“å‰å±‚åº”è¯¥æ³¨å…¥çš„DeepStackç‰¹å¾
        """
        for vision_layer, llm_layers in self.deepstack_mapping.items():
            if layer_idx in llm_layers:
                feature = deepstack_features[vision_layer]
                # å¯èƒ½éœ€è¦è°ƒæ•´shapeæˆ–ä½ç½®
                return self.prepare_deepstack_feature(feature, target_shape)
        return None

    def fuse_deepstack_features(self, hidden_states, visual_features, layer_idx):
        """
        ğŸ”¥ æ ¸å¿ƒèåˆå‡½æ•°ï¼šå±•ç¤ºä¸åŒçš„èåˆç­–ç•¥
        """
        # è·å–è§†è§‰tokençš„ä½ç½®mask
        vision_mask = self.get_vision_positions(hidden_states)

        # ========== èåˆç­–ç•¥1ï¼šç›´æ¥ç›¸åŠ ï¼ˆæœ€ç®€å•ï¼‰==========
        if self.fusion_method == 'add':
            # åªåœ¨è§†è§‰ç›¸å…³çš„ä½ç½®åŠ å…¥visual features
            hidden_states[vision_mask] = hidden_states[vision_mask] + visual_features
            return hidden_states

        # ========== èåˆç­–ç•¥2ï¼šé—¨æ§èåˆï¼ˆGated Fusionï¼‰==========
        elif self.fusion_method == 'gate':
            return self.gated_fusion(hidden_states, visual_features, vision_mask)

        # ========== èåˆç­–ç•¥3ï¼šäº¤å‰æ³¨æ„åŠ›ï¼ˆCross-Attentionï¼‰==========
        elif self.fusion_method == 'cross_attention':
            return self.cross_attention_fusion(hidden_states, visual_features)

        # ========== èåˆç­–ç•¥4ï¼šè‡ªé€‚åº”èåˆ ==========
        elif self.fusion_method == 'adaptive':
            return self.adaptive_fusion(hidden_states, visual_features, layer_idx)


class GatedFusion(nn.Module):
    """
    é—¨æ§èåˆæœºåˆ¶ï¼šå­¦ä¹ å¦‚ä½•æ··åˆæ–‡æœ¬å’Œè§†è§‰ç‰¹å¾
    """
    def __init__(self, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size

        # é—¨æ§ç½‘ç»œï¼šå†³å®šä¿ç•™å¤šå°‘åŸå§‹ç‰¹å¾vsè§†è§‰ç‰¹å¾
        self.gate_proj = nn.Linear(hidden_size * 2, hidden_size)

    def forward(self, text_hidden, vision_hidden):
        """
        text_hidden: [seq_len, hidden_size] - LLMçš„éšè—çŠ¶æ€
        vision_hidden: [vision_len, hidden_size] - Visionç‰¹å¾
        """
        # æ‹¼æ¥æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾
        combined = torch.cat([text_hidden, vision_hidden], dim=-1)

        # è®¡ç®—é—¨æ§å€¼ï¼ˆ0-1ä¹‹é—´ï¼‰
        gate = torch.sigmoid(self.gate_proj(combined))

        # åŠ æƒèåˆ
        output = gate * text_hidden + (1 - gate) * vision_hidden

        return output


class CrossAttentionFusion(nn.Module):
    """
    äº¤å‰æ³¨æ„åŠ›èåˆï¼šæ–‡æœ¬ä½œä¸ºQueryï¼Œè§†è§‰ä½œä¸ºKey/Value
    """
    def __init__(self, hidden_size, num_heads=32):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads

        self.q_proj = nn.Linear(hidden_size, hidden_size)
        self.k_proj = nn.Linear(hidden_size, hidden_size)
        self.v_proj = nn.Linear(hidden_size, hidden_size)
        self.o_proj = nn.Linear(hidden_size, hidden_size)

    def forward(self, text_hidden, vision_hidden):
        """
        ä½¿ç”¨æ–‡æœ¬queryè§†è§‰ä¿¡æ¯
        """
        batch_size, text_len, _ = text_hidden.shape
        vision_len = vision_hidden.shape[1]

        # æ–‡æœ¬ä½œä¸ºQuery
        Q = self.q_proj(text_hidden).view(batch_size, text_len, self.num_heads, self.head_dim)

        # è§†è§‰ä½œä¸ºKeyå’ŒValue
        K = self.k_proj(vision_hidden).view(batch_size, vision_len, self.num_heads, self.head_dim)
        V = self.v_proj(vision_hidden).view(batch_size, vision_len, self.num_heads, self.head_dim)

        # äº¤å‰æ³¨æ„åŠ›è®¡ç®—
        Q = Q.transpose(1, 2)  # [batch, heads, text_len, head_dim]
        K = K.transpose(1, 2)  # [batch, heads, vision_len, head_dim]
        V = V.transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)

        # è·å–è§†è§‰ä¿¡æ¯
        attn_output = torch.matmul(attn_weights, V)
        attn_output = attn_output.transpose(1, 2).reshape(batch_size, text_len, -1)

        # è¾“å‡ºæŠ•å½±å¹¶æ®‹å·®è¿æ¥
        output = self.o_proj(attn_output) + text_hidden

        return output


class AdaptiveFusion(nn.Module):
    """
    è‡ªé€‚åº”èåˆï¼šæ ¹æ®å±‚æ·±åº¦è°ƒæ•´èåˆç­–ç•¥
    """
    def __init__(self, hidden_size, num_layers=36):
        super().__init__()
        self.hidden_size = hidden_size

        # æ¯å±‚æœ‰ä¸åŒçš„èåˆæƒé‡
        self.layer_weights = nn.Parameter(torch.ones(num_layers))

        # å¯å­¦ä¹ çš„æŠ•å½±çŸ©é˜µ
        self.vision_proj = nn.ModuleList([
            nn.Linear(hidden_size, hidden_size) for _ in range(12)  # å‰12å±‚
        ])

    def forward(self, text_hidden, vision_hidden, layer_idx):
        """
        æ ¹æ®å±‚æ·±åº¦è‡ªé€‚åº”è°ƒæ•´èåˆæ–¹å¼
        """
        # è·å–å½“å‰å±‚çš„èåˆæƒé‡
        weight = torch.sigmoid(self.layer_weights[layer_idx])

        # æ—©æœŸå±‚ï¼ˆ0-3ï¼‰ï¼šæ›´å¤šä¿ç•™è§†è§‰ç»†èŠ‚
        if layer_idx < 4:
            # è§†è§‰ç‰¹å¾æŠ•å½±
            vision_projected = self.vision_proj[layer_idx](vision_hidden)
            # é«˜æƒé‡çš„è§†è§‰ç‰¹å¾
            output = text_hidden + weight * 1.5 * vision_projected

        # ä¸­æœŸå±‚ï¼ˆ4-7ï¼‰ï¼šå¹³è¡¡èåˆ
        elif layer_idx < 8:
            vision_projected = self.vision_proj[layer_idx](vision_hidden)
            # å¹³è¡¡çš„èåˆ
            output = text_hidden + weight * vision_projected

        # åæœŸå±‚ï¼ˆ8-11ï¼‰ï¼šè½»é‡èåˆ
        else:
            vision_projected = self.vision_proj[layer_idx](vision_hidden)
            # è¾ƒä½æƒé‡çš„è§†è§‰ç‰¹å¾
            output = text_hidden + weight * 0.5 * vision_projected

        return output


# ================== å®é™…Qwen3-VLçš„èåˆå®ç° ==================

def show_actual_implementation():
    """
    å±•ç¤ºQwen3-VLçš„å®é™…èåˆä»£ç 
    """
    print("=" * 70)
    print("Qwen3-VL å®é™…çš„ç‰¹å¾èåˆå®ç°")
    print("=" * 70)

    actual_code = '''
# modeling_qwen3_vl.py ä¸­çš„å®é™…å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰

class Qwen3VLForConditionalGeneration(nn.Module):
    def forward(self, input_ids, pixel_values):

        # 1. è·å–visionç‰¹å¾
        image_embeds, deepstack_embeds = self.visual(pixel_values)

        # 2. è·å–æ–‡æœ¬embedding
        inputs_embeds = self.embed_tokens(input_ids)

        # 3. è¾“å…¥å±‚èåˆï¼šæ›¿æ¢<image>ä½ç½®
        image_mask = (input_ids == self.config.image_token_id)
        inputs_embeds[image_mask] = image_embeds

        # 4. é€šè¿‡decoderå±‚
        hidden_states = inputs_embeds

        for layer_idx, decoder_layer in enumerate(self.layers):
            # æ ‡å‡†decoderå¤„ç†
            hidden_states = decoder_layer(hidden_states)

            # DeepStackèåˆï¼ˆä»…åœ¨æ—©æœŸå±‚ï¼‰
            if layer_idx < len(deepstack_embeds):
                # è·å–è§†è§‰ä½ç½®mask
                vision_mask = self._get_vision_positions(hidden_states)

                # ğŸ”¥ æ ¸å¿ƒèåˆï¼šç›´æ¥ç›¸åŠ 
                hidden_states[vision_mask] = (
                    hidden_states[vision_mask] +
                    deepstack_embeds[layer_idx]
                )

        return hidden_states

    def _get_vision_positions(self, hidden_states):
        """
        è·å–åºåˆ—ä¸­å±äºè§†è§‰çš„ä½ç½®
        """
        # åŸºäºposition_idsæˆ–attention_maskç¡®å®š
        # å“ªäº›ä½ç½®æ˜¯è§†è§‰token
        return vision_positions
    '''

    print(actual_code)
    print("\n" + "=" * 70)


def visualize_fusion_process():
    """
    å¯è§†åŒ–èåˆè¿‡ç¨‹
    """
    print("\nèåˆè¿‡ç¨‹å¯è§†åŒ–")
    print("=" * 70)

    fusion_diagram = '''
    è¾“å…¥åºåˆ—: [Text] [Image] [Text]
                â†“      â†“       â†“

    Layer 0-3 (æ¥æ”¶Vision Layer 5ç‰¹å¾):
    ------------------------------------------------
    Input:    [T T T T I I I I I T T T]  â† è¾“å…¥embedding
                      â†“ â†“ â†“ â†“ â†“
    DeepStack:        [V V V V V]        â† Layer 5ç‰¹å¾
    Fusion:   [T T T T I+V I+V I+V T T]  â† èåˆå

    Layer 4-7 (æ¥æ”¶Vision Layer 11ç‰¹å¾):
    ------------------------------------------------
    Input:    [H H H H H H H H H H H H]  â† ä¸Šå±‚è¾“å‡º
                      â†“ â†“ â†“ â†“ â†“
    DeepStack:        [V V V V V]        â† Layer 11ç‰¹å¾
    Fusion:   [H H H H H+V H+V H+V H H]  â† èåˆå

    Layer 8-11 (æ¥æ”¶Vision Layer 17ç‰¹å¾):
    ------------------------------------------------
    Input:    [H H H H H H H H H H H H]  â† ä¸Šå±‚è¾“å‡º
                      â†“ â†“ â†“ â†“ â†“
    DeepStack:        [V V V V V]        â† Layer 17ç‰¹å¾
    Fusion:   [H H H H H+V H+V H+V H H]  â† èåˆå

    Layer 12-36 (æ— DeepStack):
    ------------------------------------------------
    æ ‡å‡†Transformerå¤„ç†ï¼Œæ— é¢å¤–è§†è§‰æ³¨å…¥

    ç¬¦å·è¯´æ˜:
    T = Text embedding
    I = Image embedding (from final vision encoder)
    V = Vision features (from DeepStack)
    H = Hidden states
    + = ç‰¹å¾èåˆ(ç›¸åŠ /é—¨æ§/æ³¨æ„åŠ›)
    '''

    print(fusion_diagram)
    print("=" * 70)


def explain_fusion_benefits():
    """
    è§£é‡Šèåˆæœºåˆ¶çš„ä¼˜åŠ¿
    """
    print("\nğŸ¯ DeepStackèåˆæœºåˆ¶çš„ä¼˜åŠ¿")
    print("=" * 70)

    benefits = {
        "1. å¤šå±‚æ¬¡ç†è§£": [
            "æ—©æœŸå±‚(0-3): èåˆä½çº§è§†è§‰ç‰¹å¾ï¼Œå…³æ³¨ç»†èŠ‚",
            "ä¸­æœŸå±‚(4-7): èåˆä¸­çº§ç‰¹å¾ï¼Œç†è§£ç‰©ä½“",
            "åæœŸå±‚(8-11): èåˆé«˜çº§ç‰¹å¾ï¼ŒæŠŠæ¡è¯­ä¹‰",
            "æ·±å±‚(12-36): åŸºäºå·²èåˆç‰¹å¾åšæ¨ç†"
        ],

        "2. ä¿¡æ¯ä¿æŒ": [
            "é¿å…è§†è§‰ä¿¡æ¯åœ¨æ·±å±‚æ¶ˆå¤±",
            "ç±»ä¼¼ResNetçš„æ€æƒ³ï¼Œä½†è·¨æ¨¡æ€",
            "æ¯æ¬¡æ³¨å…¥éƒ½å¼ºåŒ–è§†è§‰ä¿¡å·"
        ],

        "3. çµæ´»èåˆ": [
            "ä¸åŒå±‚å¯ä»¥æœ‰ä¸åŒçš„èåˆç­–ç•¥",
            "å¯ä»¥å­¦ä¹ æœ€ä¼˜çš„èåˆæƒé‡",
            "è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯äº’è¡¥"
        ],

        "4. ä½ç½®æ•æ„Ÿ": [
            "åªåœ¨è§†è§‰ç›¸å…³ä½ç½®èåˆ",
            "ä¿æŒæ–‡æœ¬tokenä¸å—å¹²æ‰°",
            "ç²¾ç¡®çš„ç©ºé—´å¯¹é½"
        ]
    }

    for title, items in benefits.items():
        print(f"\n{title}:")
        for item in items:
            print(f"  â€¢ {item}")

    print("\n" + "=" * 70)


def compare_fusion_methods():
    """
    å¯¹æ¯”ä¸åŒçš„èåˆæ–¹æ³•
    """
    print("\nğŸ“Š èåˆæ–¹æ³•å¯¹æ¯”")
    print("=" * 70)

    comparison = """
    | èåˆæ–¹æ³• | å®ç° | ä¼˜ç‚¹ | ç¼ºç‚¹ |
    |---------|------|------|------|
    | ç›´æ¥ç›¸åŠ  | h = h + v | ç®€å•é«˜æ•ˆ | å¯èƒ½ä¿¡æ¯å†²çª |
    | é—¨æ§èåˆ | h = g*h + (1-g)*v | è‡ªé€‚åº”æƒé‡ | é¢å¤–å‚æ•° |
    | äº¤å‰æ³¨æ„åŠ› | h = CrossAttn(h, v) | çµæ´»äº¤äº’ | è®¡ç®—é‡å¤§ |
    | æŠ•å½±ç›¸åŠ  | h = h + Proj(v) | ç»´åº¦å¯¹é½ | éœ€è¦è®­ç»ƒæŠ•å½± |

    Qwen3-VLé€‰æ‹©ï¼šç›´æ¥ç›¸åŠ ï¼ˆç®€å•æœ‰æ•ˆï¼‰
    """

    print(comparison)
    print("=" * 70)


if __name__ == "__main__":
    # 1. å±•ç¤ºå®é™…å®ç°
    show_actual_implementation()

    # 2. å¯è§†åŒ–èåˆè¿‡ç¨‹
    visualize_fusion_process()

    # 3. è§£é‡Šèåˆä¼˜åŠ¿
    explain_fusion_benefits()

    # 4. å¯¹æ¯”èåˆæ–¹æ³•
    compare_fusion_methods()

    print("\n" + "=" * 70)
    print("âœ… æ€»ç»“ï¼šDeepStacké€šè¿‡åœ¨LLMçš„ä¸åŒæ·±åº¦æ³¨å…¥ä¸åŒå±‚æ¬¡çš„è§†è§‰ç‰¹å¾ï¼Œ")
    print("   å®ç°äº†å¤šå±‚æ¬¡ã€æ¸è¿›å¼çš„è§†è§‰-è¯­è¨€èåˆã€‚")
    print("=" * 70)